---
id: PLN-MAP-03
layer: PLN
type: MAP
phase: P5
status: active
created: 2026-02-16
updated: 2026-02-16
---
---
# Prompt Engineering Projects for CJK_KnowledgeHub

---
## **1. Prompt Evaluation & Benchmarking Framework**

**Why it matters**
Most candidates can *write* prompts. Very few can **evaluate them systematically**.

**Core idea**
A reproducible framework for:
* Defining success criteria
* Running controlled prompt experiments
* Scoring outputs across quality dimensions

**What you‚Äôd show**
* Evaluation rubric (clarity, factuality, robustness, style drift)
* A/B prompt tests
* Failure analysis logs

**Deliverables**
* `/evaluation/`
  * `rubric.md`
  * `test-cases.md`
  * `results.md`
* Jupyter notebook (optional): scoring + comparison

**Why this wins**
* Evaluation is the #1 gap in prompt engineering hiring
* Directly maps to QA, LLM Ops, and enterprise AI roles
* Almost no candidates show this convincingly

**What this project proves**
* You can define *quality* operationally
* You understand failure modes
* You can compare prompts *objectively*

**Key artifacts**
* Evaluation rubric (multi-axis)
* Test cases + controlled variations
* Scored outputs with interpretation

**Recruiter takeaway**
> ‚ÄúThis person can validate AI systems, not just produce content.‚Äù

---
### Project 1 - Playbook

#### Design

**Focus:** Thinking, not writing

**Tasks**
* Define **evaluation axes** (4‚Äì6 max)
  * e.g. Accuracy, Clarity, Robustness, Instruction Adherence
* Choose **1 realistic task** (e.g. summarization, analysis)
* Define **baseline prompt vs improved prompt**

**Checkpoint**
* One-page design rationale (bullet points)

**Done when**
* You could explain the framework verbally in 5 minutes

#### Evidence

**Focus:** Proof

**Tasks**
* Run 3‚Äì5 test cases
* Score outputs against your rubric
* Capture:
  * One success
  * One partial failure
  * One clear failure

**Artifacts**
* `rubric.md`
* `test-cases.md`
* `results.md`

**Done when**
* Differences are visible without explanation

#### Polish + Publish

**Focus:** Recruiter readability

**Tasks**
* Write README using template
* Add:
  * ‚ÄúWhat this demonstrates‚Äù
  * ‚ÄúLimitations‚Äù
* Commit + sanity check

**Result**
‚úÖ **Project 1 complete and publishable**

---
## **2. End-to-End Prompt Workflow Orchestration**

**Core idea**
A single user intent ‚Üí multiple coordinated prompts:
1. Clarify intent
2. Gather constraints
3. Generate output
4. Self-check
5. Refine

**Example use cases**
* Research synthesis
* Content production
* Technical documentation
* Job application workflows

**Deliverables**
* Workflow diagram
* Prompt chain with rationale per step
* Failure modes + mitigations

**Why this wins**
* ‚ÄúSingle prompt magic‚Äù is dead
* Enterprises think in workflows, not clever inputs
* Aligns perfectly with agentic systems without hype

**What this project proves**
* You can decompose tasks
* You understand prompt chaining
* You design for reliability and iteration

**Key artifacts**
* Workflow diagram
* Step-wise prompt chain with intent per step
* Self-check and refinement loop

**Recruiter takeaway**
> ‚ÄúThis person thinks in systems and execution paths.‚Äù

---
### Project 2 - Playbook

#### Design & Build

**Focus:** Structure

**Tasks**
* Pick **one end-to-end use case**
* Define 4‚Äì5 steps:
  1. Intent clarification
  2. Constraint gathering
  3. Draft output
  4. Self-check
  5. Refinement

**Artifacts**
* `workflow-diagram.md` (ASCII is fine)
* `prompt-chain.md`
* Example transcript

**Done when**
* Each step has a *clear reason to exist*

#### Polish + Publish

**Focus:** System clarity

**Tasks**
* Write README
* Add:
  * Failure mode (where workflow breaks)
  * Why chaining beats a single prompt
* Commit

**Result**\
‚úÖ **Project 2 complete**

---
## **3. Safety-Aware & Policy-Constrained Prompt Design**

**Why this is rare**
Most portfolios ignore safety unless explicitly asked. Enterprises never do.

**Core idea**
Demonstrate:
* Refusal patterns
* Safe redirection
* Ambiguity detection
* Escalation logic

**Example cases**
* Medical advice boundary handling
* Legal disclaimers
* Sensitive content reframing

**Deliverables**
* `safety-patterns.md`
* Before/after prompt examples
* Decision tree diagrams

**Why this wins**
* Safety literacy is rare and valuable
* Required in regulated industries
* Signals maturity and production readiness

**What this project proves**
* You understand boundaries
* You can design refusals and redirections
* You think beyond best-case outputs

**Key artifacts**
* Refusal & redirection patterns
* Ambiguity detection
* Boundary-safe rewrites

### Project 3 - Playbook

#### Design

**Focus:** Boundaries, not exhaustiveness

**Tasks**
* Select **3 safety scenarios**
  * Medical
  * Legal
  * Ambiguous harmful intent

* For each:
  * Unsafe naive prompt
  * Safe redesigned prompt
  * Explanation

**Artifacts**
* `safety-cases.md`
* `decision-logic.md`

**Done when**
* The *difference* is obvious and calm

#### Polish + Publish

**Focus:** Maturity signal

**Tasks**
* Write README
* Add ‚ÄúWhy this matters in production‚Äù
* Cross-link projects
* Final pass for tone and consistency

**Result**\
‚úÖ **All three projects recruiter-ready**

---
# KnowledgeHub Structure
KnowledgeHub/
‚îî‚îÄ prompt-engineering/
   ‚îú‚îÄ 01-prompt-evaluation-framework/
   ‚îú‚îÄ 02-prompt-workflow-orchestration/
   ‚îî‚îÄ 03-safety-aware-prompt-design/

Each project contains:
* `README.md` ‚Üí executive overview
	* how the three projects relate
	* short ‚ÄúHow I approach prompt engineering‚Äù note
* `design-rationale.md` ‚Üí your thinking
* `examples/` ‚Üí proof
* `failure-modes.md` ‚Üí credibility
* `lessons-learned.md` ‚Üí maturity

---
# Strategic Positioning

## üß† Strategic positioning (important)

These are **not tutorials**. They are **design case studies**.

Tone:
* Calm
* Analytical
* Explicit about tradeoffs

This aligns perfectly with:
* AI Systems Engineer
* Prompt Engineer (Senior)
* AI Workflow Designer
* Responsible AI / LLM Ops roles

---
# Execution Plan

**Phase 1 - Pick **3 projects** to build**
* Evaluation Framework
* Workflow Orchestration
* Safety-Aware & Policy Constrained Prompt Design

**Phase 2 ‚Äì Build**
* Minimal but *complete*
* Real examples > theoretical breadth

**Phase 3 ‚Äì Polish**
* Clear diagrams
* Tight language
* Explicit ‚ÄúWhat this shows recruiters‚Äù section

**Phase 4 ‚Äì Surface**
* Link selectively from:
  * CV
  * LinkedIn ‚ÄúFeatured‚Äù
  * GitHub profile README

---
# Adversarial Review | Claude

***The following is what Claude had to say about the project proposals***

---
## Assessment of the Three Project Ideas

### Project 1: Prompt Evaluation & Benchmarking Framework

#### Strengths

* **Highly differentiating** - Evaluation is genuinely rare in portfolios. Most candidates show prompt writing; very few show systematic quality assessment.
* **Directly applicable** - Maps cleanly to enterprise AI QA, LLM Ops, and validation roles.
* **Demonstrates rigor** - Shows you think operationally about "what makes a good output" rather than subjectively.
* **Scalable concept** - Framework can be applied to any domain (summarization, analysis, generation).

#### Improvements

* **Specify the domain** - "Evaluation framework" is abstract. Anchor it to one use case (e.g., "Evaluation Framework for Technical Documentation Summarization") to make it concrete.
* **Include comparison methodology** - Not just scoring individual outputs, but comparing prompt variants (A/B testing approach).
* **Add failure taxonomy** - Categorize *why* prompts fail (instruction drift, hallucination, format non-compliance, etc.).

#### What makes this succeed

Clear before/after examples where your rubric reveals meaningful differences between prompts that look similar on the surface.

***

### Project 2: End-to-End Prompt Workflow Orchestration

#### Strengths

* **Systems thinking** - Shows you understand decomposition and multi-step reasoning.
* **Enterprise-relevant** - Companies don't use single prompts; they use chains. This is realistic.
* **Demonstrates reliability design** - Self-check and refinement loops show production thinking.
* **Differentiates from "clever prompt" candidates** - Positions you as workflow designer, not just prompt writer.

#### Improvements

* **Choose the use case carefully** - "Research synthesis" is vague. Be specific: "Research synthesis for competitive analysis reports" or "Technical documentation from code + README + issues."
* **Show failure mode handling** - What happens when step 3 produces garbage? How does the workflow recover? This is where orchestration proves its value.
* **Clarify the refinement mechanism** - How does the self-check actually work? What criteria trigger refinement vs. acceptance?
* **Consider showing two examples** - One where the workflow succeeds smoothly, one where it catches and corrects an error.

#### What makes this succeed

Demonstrating that orchestration *catches problems* that a single prompt would miss. The value is in robustness, not just organization.

---
### Project 3: Safety-Aware & Policy-Constrained Prompt Design

#### Strengths

* **Maturity signal** - Most portfolios ignore safety entirely. Including it shows production readiness.
* **EU market relevance** - GDPR, AI Act considerations make this especially valuable in your geography.
* **Demonstrates boundary thinking** - Shows you understand where AI systems should refuse, redirect, or escalate.
* **Undervalued skill** - Companies need this but candidates rarely showcase it.

#### Improvements

* **Focus on detection, not just refusal** - Don't just show "safe vs unsafe prompt." Show how to *detect ambiguous intent* and handle it gracefully.
* **Include escalation paths** - When does the system refuse? When does it ask for clarification? When does it provide partial information with caveats?
* **Add realistic gray areas** - Medical/legal are good, but also include: financial advice, identity verification, minors protection, content moderation edge cases.
* **Show the tradeoff** - Overly cautious = bad UX. Too permissive = risk. How do you balance?

#### What makes this succeed

Demonstrating nuanced judgment, not just binary safe/unsafe classification. The quality is in handling ambiguity.

---
## How the Three Projects Work Together

**Collective narrative:** "I design, orchestrate, and validate AI systems with safety and quality built in."

* **Project 1** ‚Üí "I can measure quality systematically"
* **Project 2** ‚Üí "I can architect reliable multi-step workflows"
* **Project 3** ‚Üí "I can handle edge cases and boundaries responsibly"

**Coherence check:** ‚úÖ These complement each other without overlap. Each proves a different skill cluster.

---
## Overall Verdict on the Project Ideas

**Are these good ideas?** ‚Üí **Yes**, with the improvements noted above.

**Do they differentiate you?** ‚Üí **Yes**, especially Project 1. Most prompt engineering portfolios show "I wrote good prompts" not "I evaluated and iterated systematically."

**Are they relevant to your target roles?** ‚Üí **Yes** for Prompt Engineering and AI Workflows. Less so for pure automation/documentation, but they establish technical credibility.

**What's the biggest risk?** ‚Üí These could appear theoretical if not grounded in concrete, specific examples. The fix: Be ruthlessly specific about domains, use cases, and examples.

---
## Setting You Up for Success

### For each project, you need:

1. **One specific domain/use case** - Not "summarization" but "summarizing customer support tickets for escalation reports"
2. **Concrete examples** - Not "tested 5 cases" but showing actual inputs/outputs with your scoring/analysis
3. **Failure analysis** - Not just what worked, but what broke and why (this is where expertise shows)
4. **Clear takeaway** - Each project should answer: "What capability does this prove I have?"

### The quality bar

Your projects should make a recruiter think:

* Project 1: "This person can validate AI systems objectively"
* Project 2: "This person understands production workflow design"
* Project 3: "This person thinks about risk and boundaries proactively"

Not:

* "Nice theoretical framework"
* "Interesting ideas"
* "Good documentation"

**The ideas are solid. The execution will determine whether they land.**


